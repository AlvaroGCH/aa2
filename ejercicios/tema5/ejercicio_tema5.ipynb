{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb1c742",
   "metadata": {},
   "source": [
    "# Ejercicio 3: Visualización de Atención con BertViz\n",
    "\n",
    "## Parte A: Instalación y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff6c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalación de paquetes necesarios\n",
    "%pip install bertviz transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c201075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alvar\\OneDrive\\Universidad\\Data\\2º Cuatri\\AA 2\\ejercicios\\tema5\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\alvar\\OneDrive\\Universidad\\Data\\2º Cuatri\\AA 2\\ejercicios\\tema5\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alvar\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1277.99it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-multilingual-cased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente\n",
      "Capas: 12\n",
      "Cabezas de atención por capa: 12\n",
      "Dimensión del modelo: 768\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "print(\"Modelo cargado correctamente\")\n",
    "print(f\"Capas: {model.config.num_hidden_layers}\")\n",
    "print(f\"Cabezas de atención por capa: {model.config.num_attention_heads}\")\n",
    "print(f\"Dimensión del modelo: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326efe5",
   "metadata": {},
   "source": [
    "## Parte B: Función de análisis de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56de30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention(sentence, model, tokenizer):\n",
    "    \"\"\"Analiza los patrones de atención para una oración.\"\"\"\n",
    "    # Tokenizar\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extraer atención\n",
    "    attentions = outputs.attentions\n",
    "    \n",
    "    print(f\"Oración: {sentence}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Número de capas: {len(attentions)}\")\n",
    "    print(f\"Forma de atención por capa: {attentions[0].shape}\")\n",
    "    \n",
    "    return attentions, tokens\n",
    "\n",
    "def print_attention_weights(attentions, tokens, layer, head, threshold=0.15):\n",
    "    \"\"\"Imprime los pesos de atención de una cabeza específica.\"\"\"\n",
    "    att = attentions[layer][0, head].numpy()\n",
    "    print(f\"\\n=== Capa {layer}, Cabeza {head} ===\")\n",
    "    print(f\"{'':>15}\", end=\"\")\n",
    "    for t in tokens:\n",
    "        print(f\"{t:>12}\", end=\"\")\n",
    "    print()\n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"{token:>15}\", end=\"\")\n",
    "        for j in range(len(tokens)):\n",
    "            val = att[i][j]\n",
    "            # Resaltar valores altos con *\n",
    "            marker = \"*\" if val > threshold else \" \"\n",
    "            print(f\"{val:>11.4f}{marker}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "def find_strong_connections(attentions, tokens, source_word, target_word):\n",
    "    \"\"\"Encuentra capas y cabezas donde source_word atiende fuertemente a target_word.\"\"\"\n",
    "    connections = []\n",
    "    \n",
    "    # Buscar índices de los tokens (búsqueda flexible con subpalabras)\n",
    "    source_indices = []\n",
    "    target_indices = []\n",
    "    \n",
    "    for i, t in enumerate(tokens):\n",
    "        # Eliminar ## y convertir a minúsculas para búsqueda\n",
    "        clean_token = t.replace(\"##\", \"\").lower()\n",
    "        if source_word.lower() in clean_token or clean_token in source_word.lower():\n",
    "            source_indices.append(i)\n",
    "        if target_word.lower() in clean_token or clean_token in target_word.lower():\n",
    "            target_indices.append(i)\n",
    "    \n",
    "    if not source_indices or not target_indices:\n",
    "        print(f\"Source '{source_word}': {source_indices}, Target '{target_word}': {target_indices}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Tokens de '{source_word}': {[tokens[i] for i in source_indices]}\")\n",
    "    print(f\"Tokens de '{target_word}': {[tokens[i] for i in target_indices]}\")\n",
    "    \n",
    "    # Buscar en todas las capas y cabezas\n",
    "    for layer_idx, layer_att in enumerate(attentions):\n",
    "        for head_idx in range(layer_att.shape[1]):\n",
    "            att = layer_att[0, head_idx].numpy()\n",
    "            for src_idx in source_indices:\n",
    "                for tgt_idx in target_indices:\n",
    "                    weight = att[src_idx, tgt_idx]\n",
    "                    if weight > 0.15:  # Umbral de atención fuerte\n",
    "                        connections.append({\n",
    "                            'layer': layer_idx,\n",
    "                            'head': head_idx,\n",
    "                            'weight': weight,\n",
    "                            'source_idx': src_idx,\n",
    "                            'target_idx': tgt_idx,\n",
    "                            'source_token': tokens[src_idx],\n",
    "                            'target_token': tokens[tgt_idx]\n",
    "                        })\n",
    "    \n",
    "    # Ordenar por peso descendente\n",
    "    connections.sort(key=lambda x: x['weight'], reverse=True)\n",
    "    return connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b0ed3",
   "metadata": {},
   "source": [
    "## Parte C: Análisis de Patrones\n",
    "\n",
    "### Oración 1 - Correferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f53760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración: El gato se sentó en la alfombra porque estaba cansado\n",
      "Tokens: ['[CLS]', 'El', 'ga', '##to', 'se', 'sent', '##ó', 'en', 'la', 'al', '##fo', '##mbra', 'porque', 'estaba', 'can', '##sado', '[SEP]']\n",
      "Número de capas: 12\n",
      "Forma de atención por capa: torch.Size([1, 12, 17, 17])\n",
      "\n",
      "--- Buscando correferencia: estaba/cansado -> gato ---\n",
      "Tokens de 'estaba': ['estaba']\n",
      "Tokens de 'gato': ['ga', '##to']\n",
      "Tokens de 'cansado': ['can', '##sado']\n",
      "Tokens de 'gato': ['ga', '##to']\n",
      "\n",
      "Top 5 conexiones 'estaba' -> 'gato':\n",
      "Capa  8, Cabeza  2: 0.3982\n",
      "\n",
      "Top 5 conexiones 'cansado' -> 'gato':\n",
      "Capa  8, Cabeza  2: 0.3475\n",
      "Capa  8, Cabeza  2: 0.3121\n",
      "Capa 10, Cabeza  1: 0.2259\n",
      "Capa  8, Cabeza  2: 0.1919\n",
      "Capa  8, Cabeza  7: 0.1786\n"
     ]
    }
   ],
   "source": [
    "# Oración 1: Correferencia\n",
    "sentence_1 = \"El gato se sentó en la alfombra porque estaba cansado\"\n",
    "attentions_1, tokens_1 = analyze_attention(sentence_1, model, tokenizer)\n",
    "\n",
    "# Buscar conexiones entre \"estaba\"/\"cansado\" y \"gato\"\n",
    "print(\"\\n--- Buscando correferencia: estaba/cansado -> gato ---\")\n",
    "connections_estaba = find_strong_connections(attentions_1, tokens_1, \"estaba\", \"gato\")\n",
    "connections_cansado = find_strong_connections(attentions_1, tokens_1, \"cansado\", \"gato\")\n",
    "\n",
    "print(\"\\nTop 5 conexiones 'estaba' -> 'gato':\")\n",
    "for conn in connections_estaba[:5]:\n",
    "    print(f\"Capa {conn['layer']:2d}, Cabeza {conn['head']:2d}: {conn['weight']:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 conexiones 'cansado' -> 'gato':\")\n",
    "for conn in connections_cansado[:5]:\n",
    "    print(f\"Capa {conn['layer']:2d}, Cabeza {conn['head']:2d}: {conn['weight']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3a271",
   "metadata": {},
   "source": [
    "### Oración 2 - Estructura sintáctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890e3d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración: Los estudiantes que aprobaron el examen celebraron con sus amigos\n",
      "Tokens: ['[CLS]', 'Los', 'estudiantes', 'que', 'ap', '##ro', '##bar', '##on', 'el', 'examen', 'celebrar', '##on', 'con', 'sus', 'amigos', '[SEP]']\n",
      "Número de capas: 12\n",
      "Forma de atención por capa: torch.Size([1, 12, 16, 16])\n",
      "\n",
      "--- Buscando relación sintáctica: celebraron -> estudiantes ---\n",
      "Tokens de 'celebraron': ['##ro', '##on', 'el', 'celebrar', '##on']\n",
      "Tokens de 'estudiantes': ['estudiantes']\n",
      "\n",
      "Top 5 conexiones 'celebraron' -> 'estudiantes':\n",
      "Capa  0, Cabeza  1: 0.6613\n",
      "Capa  8, Cabeza  2: 0.5466\n",
      "Capa  9, Cabeza  4: 0.4172\n",
      "Capa 10, Cabeza  1: 0.4094\n",
      "Capa  0, Cabeza  2: 0.3427\n"
     ]
    }
   ],
   "source": [
    "# Oración 2: Estructura sintáctica\n",
    "sentence_2 = \"Los estudiantes que aprobaron el examen celebraron con sus amigos\"\n",
    "attentions_2, tokens_2 = analyze_attention(sentence_2, model, tokenizer)\n",
    "\n",
    "# Buscar conexiones entre \"celebraron\" y \"estudiantes\"\n",
    "print(\"\\n--- Buscando relación sintáctica: celebraron -> estudiantes ---\")\n",
    "connections_2 = find_strong_connections(attentions_2, tokens_2, \"celebraron\", \"estudiantes\")\n",
    "\n",
    "print(\"\\nTop 5 conexiones 'celebraron' -> 'estudiantes':\")\n",
    "for conn in connections_2[:5]:\n",
    "    print(f\"Capa {conn['layer']:2d}, Cabeza {conn['head']:2d}: {conn['weight']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba44926",
   "metadata": {},
   "source": [
    "### Oración 3 - Relaciones a larga distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fc74bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración: La empresa que fundaron en Madrid hace diez años finalmente cerró\n",
      "Tokens: ['[CLS]', 'La', 'empresa', 'que', 'fundar', '##on', 'en', 'Madrid', 'hace', 'diez', 'años', 'finalmente', 'ce', '##rr', '##ó', '[SEP]']\n",
      "Número de capas: 12\n",
      "Forma de atención por capa: torch.Size([1, 12, 16, 16])\n",
      "\n",
      "--- Buscando relación a larga distancia: cerró -> empresa ---\n",
      "Tokens de 'cerró': ['ce', '##rr', '##ó']\n",
      "Tokens de 'empresa': ['empresa']\n",
      "\n",
      "Top 5 conexiones 'cerró' -> 'empresa':\n",
      "Capa  7, Cabeza  5: 0.6073\n",
      "Capa  7, Cabeza  5: 0.5093\n",
      "Capa  9, Cabeza  4: 0.4473\n",
      "Capa  8, Cabeza  2: 0.4168\n",
      "Capa  8, Cabeza  2: 0.4126\n"
     ]
    }
   ],
   "source": [
    "# Oración 3: Relaciones a larga distancia\n",
    "sentence_3 = \"La empresa que fundaron en Madrid hace diez años finalmente cerró\"\n",
    "attentions_3, tokens_3 = analyze_attention(sentence_3, model, tokenizer)\n",
    "\n",
    "# Buscar conexiones entre \"cerró\" y \"empresa\"\n",
    "print(\"\\n--- Buscando relación a larga distancia: cerró -> empresa ---\")\n",
    "connections_3 = find_strong_connections(attentions_3, tokens_3, \"cerró\", \"empresa\")\n",
    "\n",
    "print(\"\\nTop 5 conexiones 'cerró' -> 'empresa':\")\n",
    "for conn in connections_3[:5]:\n",
    "    print(f\"Capa {conn['layer']:2d}, Cabeza {conn['head']:2d}: {conn['weight']:.4f}\")\n",
    "\n",
    "if len(connections_3) < 5:\n",
    "    print(f\"\\nSolo se encontraron {len(connections_3)} conexiones fuertes (>0.2)\")\n",
    "    print(\"Las relaciones a larga distancia son más difíciles de capturar cuando hay cláusulas relativas intermedias.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec903936",
   "metadata": {},
   "source": [
    "### Oración 4 - Comparación de idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd38674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración: El banco está cerca del río\n",
      "Tokens: ['[CLS]', 'El', 'banco', 'está', 'cerca', 'del', 'río', '[SEP]']\n",
      "Número de capas: 12\n",
      "Forma de atención por capa: torch.Size([1, 12, 8, 8])\n",
      "\n",
      "============================================================\n",
      "Oración: The bank is near the river\n",
      "Tokens: ['[CLS]', 'The', 'bank', 'is', 'near', 'the', 'river', '[SEP]']\n",
      "Número de capas: 12\n",
      "Forma de atención por capa: torch.Size([1, 12, 8, 8])\n",
      "\n",
      "--- Analizando 'banco' en español ---\n",
      "Índice de 'banco': 2\n",
      "\n",
      "Patrones de atención del token 'banco' en capa 6, cabeza 4:\n",
      "\n",
      "=== Capa 6, Cabeza 4 ===\n",
      "                      [CLS]          El       banco        está       cerca         del         río       [SEP]\n",
      "          [CLS]     0.0433      0.1753*     0.1935*     0.0695      0.0701      0.0918      0.2114*     0.1451 \n",
      "             El     0.0603      0.2836*     0.3254*     0.0743      0.0313      0.0158      0.0880      0.1214 \n",
      "          banco     0.0760      0.3610*     0.1910*     0.1301      0.0518      0.0191      0.0812      0.0900 \n",
      "           está     0.0084      0.4312*     0.3413*     0.0798      0.0452      0.0172      0.0399      0.0368 \n",
      "          cerca     0.0130      0.3581*     0.3392*     0.1455      0.0363      0.0278      0.0403      0.0398 \n",
      "            del     0.0204      0.2158*     0.2008*     0.3722*     0.0598      0.0450      0.0522      0.0336 \n",
      "            río     0.0358      0.1217      0.1258      0.3058*     0.1432      0.1283      0.1067      0.0327 \n",
      "          [SEP]     0.0161      0.1190      0.0977      0.1656*     0.1989*     0.1374      0.1691*     0.0960 \n",
      "\n",
      "--- Analizando 'bank' en inglés ---\n",
      "Índice de 'bank': 2\n",
      "\n",
      "Patrones de atención del token 'bank' en capa 6, cabeza 4:\n",
      "\n",
      "=== Capa 6, Cabeza 4 ===\n",
      "                      [CLS]         The        bank          is        near         the       river       [SEP]\n",
      "          [CLS]     0.0572      0.1643*     0.1868*     0.0878      0.0568      0.0482      0.2271*     0.1718*\n",
      "            The     0.0593      0.2699*     0.2816*     0.1138      0.0445      0.0157      0.1040      0.1111 \n",
      "           bank     0.1205      0.2885*     0.1944*     0.1604*     0.0595      0.0158      0.0758      0.0851 \n",
      "             is     0.0130      0.4261*     0.3673*     0.0706      0.0396      0.0067      0.0337      0.0430 \n",
      "           near     0.0145      0.3496*     0.2945*     0.2158*     0.0493      0.0123      0.0314      0.0325 \n",
      "            the     0.0164      0.1833*     0.1309      0.4731*     0.0912      0.0293      0.0442      0.0315 \n",
      "          river     0.0375      0.1148      0.1047      0.2811*     0.1913*     0.1142      0.1159      0.0406 \n",
      "          [SEP]     0.0345      0.1091      0.1155      0.1499      0.2010*     0.0648      0.1871*     0.1381 \n"
     ]
    }
   ],
   "source": [
    "# Oración 4: Comparación de idiomas\n",
    "sentence_es = \"El banco está cerca del río\"\n",
    "sentence_en = \"The bank is near the river\"\n",
    "\n",
    "attentions_es, tokens_es = analyze_attention(sentence_es, model, tokenizer)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "attentions_en, tokens_en = analyze_attention(sentence_en, model, tokenizer)\n",
    "\n",
    "# Analizar \"banco\"/\"bank\"\n",
    "print(\"\\n--- Analizando 'banco' en español ---\")\n",
    "banco_idx = [i for i, t in enumerate(tokens_es) if \"banco\" in t.lower()]\n",
    "if banco_idx:\n",
    "    print(f\"Índice de 'banco': {banco_idx[0]}\")\n",
    "    print(\"\\nPatrones de atención del token 'banco' en capa 6, cabeza 4:\")\n",
    "    print_attention_weights(attentions_es, tokens_es, 6, 4)\n",
    "\n",
    "print(\"\\n--- Analizando 'bank' en inglés ---\")\n",
    "bank_idx = [i for i, t in enumerate(tokens_en) if \"bank\" in t.lower()]\n",
    "if bank_idx:\n",
    "    print(f\"Índice de 'bank': {bank_idx[0]}\")\n",
    "    print(\"\\nPatrones de atención del token 'bank' en capa 6, cabeza 4:\")\n",
    "    print_attention_weights(attentions_en, tokens_en, 6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f2932",
   "metadata": {},
   "source": [
    "## Análisis general de patrones por capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0222a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CAPAS TEMPRANAS (0-3) ===\n",
      "\n",
      "Capa 0, Cabeza 0:\n",
      "\n",
      "=== Capa 0, Cabeza 0 ===\n",
      "                      [CLS]          El          ga        ##to          se        sent         ##ó          en          la          al        ##fo      ##mbra      porque      estaba         can      ##sado       [SEP]\n",
      "          [CLS]     0.1135      0.0070      0.0012      0.0008      0.0027      0.0015      0.0105      0.0067      0.0018      0.0020      0.0011      0.0021      0.0080      0.0507      0.0054      0.0050      0.7799*\n",
      "             El     0.0222      0.3850*     0.0084      0.0417      0.0679      0.0221      0.0282      0.0472      0.1077      0.0539      0.0124      0.0248      0.0517      0.0316      0.0254      0.0168      0.0530 \n",
      "             ga     0.0310      0.0582      0.1371      0.0213      0.0467      0.0217      0.0232      0.0799      0.0570      0.0263      0.0284      0.0543      0.0746      0.1509      0.0484      0.0341      0.1069 \n",
      "           ##to     0.1205      0.0273      0.0423      0.0525      0.0597      0.0138      0.0313      0.0196      0.0191      0.0172      0.0618      0.0822      0.1319      0.1341      0.0571      0.0411      0.0885 \n",
      "             se     0.3578*     0.0092      0.0168      0.0381      0.0207      0.0127      0.0073      0.0164      0.0204      0.0110      0.0048      0.0884      0.1328      0.0908      0.0300      0.0542      0.0887 \n",
      "           sent     0.0087      0.0595      0.0065      0.0078      0.1259      0.0112      0.0034      0.0851      0.0645      0.0366      0.0020      0.0056      0.2301*     0.2811*     0.0232      0.0231      0.0256 \n",
      "            ##ó     0.0567      0.0457      0.0110      0.0063      0.0383      0.0151      0.1253      0.0278      0.0179      0.0198      0.0113      0.0155      0.2646*     0.2020*     0.0092      0.0423      0.0914 \n",
      "             en     0.1718      0.0470      0.0156      0.0225      0.0495      0.0192      0.0080      0.1413      0.1184      0.0232      0.0150      0.0464      0.0376      0.0801      0.0692      0.0204      0.1149 \n",
      "             la     0.2657*     0.0661      0.0114      0.0353      0.0257      0.0166      0.0327      0.0607      0.1821      0.0366      0.0263      0.0338      0.0228      0.0282      0.0300      0.0244      0.1015 \n",
      "             al     0.0509      0.1636      0.0053      0.0391      0.0241      0.0202      0.0167      0.0244      0.1264      0.0927      0.0122      0.0274      0.0692      0.1234      0.0789      0.0444      0.0813 \n",
      "           ##fo     0.1864      0.0638      0.0390      0.0326      0.0701      0.0301      0.0339      0.0681      0.0301      0.0142      0.0793      0.0425      0.0710      0.0688      0.0151      0.0340      0.1211 \n",
      "         ##mbra     0.3763*     0.0098      0.0205      0.0279      0.0152      0.0378      0.0136      0.0176      0.0094      0.0036      0.0212      0.1546      0.0379      0.0408      0.0350      0.0275      0.1515 \n",
      "         porque     0.5005*     0.0094      0.0104      0.0389      0.0216      0.0356      0.0351      0.0197      0.0124      0.0066      0.0111      0.0402      0.0586      0.0753      0.0168      0.0391      0.0687 \n",
      "         estaba     0.1001      0.0065      0.0373      0.0330      0.0311      0.1004      0.0138      0.0228      0.0201      0.0182      0.0142      0.0413      0.0302      0.1632      0.0243      0.0320      0.3115*\n",
      "            can     0.0519      0.0210      0.0160      0.0142      0.0790      0.0211      0.0273      0.0408      0.0260      0.0210      0.0156      0.0723      0.1150      0.0981      0.2657*     0.0100      0.1048 \n",
      "         ##sado     0.0450      0.0774      0.0333      0.0185      0.0399      0.0143      0.0463      0.0081      0.0310      0.0242      0.0221      0.0662      0.1838      0.0564      0.0367      0.1099      0.1869 \n",
      "          [SEP]     0.0005      0.0002      0.0005      0.0011      0.0005      0.0001      0.0010      0.0003      0.0001      0.0004      0.0015      0.0003      0.0001      0.0003      0.0003      0.0001      0.9927*\n",
      "\n",
      "Capa 2, Cabeza 0:\n",
      "\n",
      "=== Capa 2, Cabeza 0 ===\n",
      "                      [CLS]          El          ga        ##to          se        sent         ##ó          en          la          al        ##fo      ##mbra      porque      estaba         can      ##sado       [SEP]\n",
      "          [CLS]     0.9758*     0.0007      0.0009      0.0013      0.0018      0.0006      0.0015      0.0013      0.0011      0.0008      0.0006      0.0009      0.0006      0.0012      0.0012      0.0010      0.0088 \n",
      "             El     0.1100      0.0961      0.0527      0.0216      0.0294      0.0356      0.0373      0.0430      0.0666      0.0298      0.0265      0.0553      0.0840      0.0533      0.0349      0.0599      0.1639 \n",
      "             ga     0.1541      0.0693      0.1080      0.0304      0.0236      0.0287      0.0379      0.0292      0.0408      0.0485      0.0506      0.0594      0.0240      0.0689      0.0516      0.0649      0.1101 \n",
      "           ##to     0.2464*     0.0415      0.0702      0.0233      0.0701      0.0204      0.0339      0.0282      0.0412      0.0211      0.0465      0.0421      0.0339      0.0574      0.0421      0.0348      0.1467 \n",
      "             se     0.4197*     0.0603      0.0296      0.0204      0.0369      0.0233      0.0567      0.0184      0.0224      0.0163      0.0194      0.0263      0.0452      0.0405      0.0095      0.0214      0.1336 \n",
      "           sent     0.3271*     0.1043      0.0429      0.0159      0.0360      0.0167      0.0592      0.0263      0.0561      0.0160      0.0183      0.0396      0.0559      0.0445      0.0152      0.0237      0.1024 \n",
      "            ##ó     0.4174*     0.0420      0.0258      0.0173      0.0517      0.0196      0.0554      0.0222      0.0314      0.0090      0.0177      0.0432      0.0314      0.0409      0.0097      0.0237      0.1416 \n",
      "             en     0.2047*     0.0591      0.0539      0.0286      0.0408      0.0379      0.0427      0.0285      0.0457      0.0227      0.0357      0.0459      0.0499      0.0426      0.0383      0.0399      0.1830 \n",
      "             la     0.2281*     0.0765      0.0493      0.0310      0.0302      0.0292      0.0394      0.0424      0.0372      0.0289      0.0301      0.0542      0.0604      0.0414      0.0276      0.0382      0.1559 \n",
      "             al     0.2723*     0.0265      0.0646      0.0347      0.0384      0.0257      0.0182      0.0326      0.0326      0.0354      0.0576      0.0505      0.0150      0.0204      0.0364      0.0247      0.2144*\n",
      "           ##fo     0.6671*     0.0201      0.0307      0.0179      0.0207      0.0112      0.0122      0.0172      0.0214      0.0237      0.0149      0.0205      0.0099      0.0070      0.0103      0.0193      0.0761 \n",
      "         ##mbra     0.2998*     0.0158      0.0498      0.0185      0.0339      0.0138      0.1282      0.0162      0.0164      0.0258      0.0817      0.0268      0.0116      0.1078      0.0162      0.0300      0.1078 \n",
      "         porque     0.4440*     0.0408      0.0212      0.0322      0.0571      0.0220      0.0353      0.0376      0.0240      0.0132      0.0199      0.0145      0.0219      0.0250      0.0080      0.0180      0.1654 \n",
      "         estaba     0.3373*     0.0538      0.0473      0.0252      0.0347      0.0169      0.0783      0.0312      0.0315      0.0200      0.0359      0.0621      0.0242      0.0402      0.0094      0.0325      0.1197 \n",
      "            can     0.1087      0.0327      0.1242      0.0495      0.0149      0.0469      0.0196      0.0203      0.0300      0.0370      0.0723      0.1176      0.0155      0.0454      0.0904      0.0461      0.1289 \n",
      "         ##sado     0.3525*     0.0387      0.0475      0.0286      0.0385      0.0209      0.0392      0.0215      0.0194      0.0224      0.0408      0.0397      0.0224      0.0984      0.0348      0.0241      0.1107 \n",
      "          [SEP]     0.4762*     0.0758      0.0317      0.0269      0.0400      0.0182      0.0670      0.0248      0.0294      0.0169      0.0198      0.0184      0.0293      0.0505      0.0154      0.0231      0.0368 \n",
      "\n",
      "\n",
      "=== CAPAS PROFUNDAS (9-11) ===\n",
      "\n",
      "Capa 9, Cabeza 0:\n",
      "\n",
      "=== Capa 9, Cabeza 0 ===\n",
      "                      [CLS]          El          ga        ##to          se        sent         ##ó          en          la          al        ##fo      ##mbra      porque      estaba         can      ##sado       [SEP]\n",
      "          [CLS]     0.1541      0.0602      0.0174      0.0353      0.0764      0.0232      0.0685      0.0234      0.0399      0.0317      0.0152      0.0133      0.0404      0.0194      0.0089      0.0279      0.3447*\n",
      "             El     0.2229*     0.2432*     0.0579      0.0360      0.0306      0.0018      0.0216      0.0024      0.0027      0.0010      0.0003      0.0008      0.0242      0.0038      0.0004      0.0030      0.3476*\n",
      "             ga     0.0284      0.0372      0.3812*     0.1943      0.0043      0.0035      0.0070      0.0015      0.0005      0.0036      0.0035      0.0050      0.0083      0.0030      0.0019      0.0835      0.2336*\n",
      "           ##to     0.0412      0.0197      0.5300*     0.2439*     0.0073      0.0058      0.0233      0.0016      0.0007      0.0030      0.0034      0.0057      0.0040      0.0047      0.0015      0.0254      0.0790 \n",
      "             se     0.2005*     0.0577      0.0252      0.0286      0.1482      0.0633      0.2389*     0.0283      0.0159      0.0029      0.0006      0.0025      0.0378      0.0071      0.0004      0.0037      0.1386 \n",
      "           sent     0.0196      0.0062      0.0069      0.0072      0.1409      0.3062*     0.2616*     0.0584      0.0106      0.0100      0.0027      0.0045      0.0275      0.0102      0.0013      0.0447      0.0815 \n",
      "            ##ó     0.1023      0.0252      0.0121      0.0222      0.1566      0.1468      0.2201*     0.0405      0.0155      0.0069      0.0018      0.0062      0.0467      0.0158      0.0016      0.0221      0.1576 \n",
      "             en     0.0546      0.0237      0.0050      0.0043      0.1377      0.0616      0.0936      0.1770      0.1061      0.1253      0.0111      0.0331      0.0258      0.0095      0.0012      0.0059      0.1244 \n",
      "             la     0.1664      0.0339      0.0021      0.0029      0.0874      0.0114      0.0528      0.0874      0.0834      0.0493      0.0032      0.0172      0.0510      0.0149      0.0012      0.0076      0.3280*\n",
      "             al     0.0779      0.0116      0.0240      0.0217      0.0107      0.0170      0.0205      0.0194      0.0127      0.0878      0.0731      0.3143*     0.0118      0.0018      0.0007      0.0579      0.2370*\n",
      "           ##fo     0.0364      0.0026      0.0181      0.0104      0.0037      0.0130      0.0102      0.0077      0.0054      0.1045      0.0889      0.4577*     0.0088      0.0027      0.0017      0.1430      0.0853 \n",
      "         ##mbra     0.0392      0.0044      0.0415      0.0242      0.0129      0.0191      0.0198      0.0101      0.0053      0.2345*     0.1023      0.2514*     0.0187      0.0094      0.0021      0.0752      0.1300 \n",
      "         porque     0.0914      0.0171      0.0064      0.0113      0.0370      0.0121      0.0343      0.0056      0.0036      0.0049      0.0025      0.0020      0.3730*     0.0523      0.0080      0.0376      0.3009*\n",
      "         estaba     0.0618      0.0065      0.0041      0.0064      0.0089      0.0058      0.0196      0.0064      0.0045      0.0034      0.0022      0.0032      0.0937      0.3514*     0.0216      0.2586*     0.1420 \n",
      "            can     0.0065      0.0021      0.0039      0.0033      0.0012      0.0036      0.0023      0.0017      0.0013      0.0112      0.0083      0.0060      0.0076      0.0196      0.0683      0.8001*     0.0531 \n",
      "         ##sado     0.0419      0.0008      0.0047      0.0103      0.0026      0.0037      0.0042      0.0015      0.0007      0.0090      0.0066      0.0051      0.0155      0.1089      0.1252      0.6234*     0.0358 \n",
      "          [SEP]     0.3782*     0.0351      0.0105      0.0350      0.0549      0.0070      0.0500      0.0059      0.0050      0.0053      0.0043      0.0044      0.0880      0.0259      0.0142      0.0330      0.2434*\n",
      "\n",
      "Capa 11, Cabeza 0:\n",
      "\n",
      "=== Capa 11, Cabeza 0 ===\n",
      "                      [CLS]          El          ga        ##to          se        sent         ##ó          en          la          al        ##fo      ##mbra      porque      estaba         can      ##sado       [SEP]\n",
      "          [CLS]     0.3173*     0.1177      0.0270      0.0194      0.0597      0.0307      0.0351      0.0169      0.0231      0.0065      0.0047      0.0207      0.1058      0.0121      0.0168      0.0246      0.1619 \n",
      "             El     0.1510      0.3502*     0.0264      0.0262      0.0481      0.0140      0.0171      0.0040      0.0084      0.0021      0.0032      0.0074      0.1116      0.0124      0.0160      0.0254      0.1766 \n",
      "             ga     0.2035*     0.1284      0.0581      0.0410      0.0445      0.0695      0.0403      0.0067      0.0192      0.0092      0.0113      0.0161      0.0509      0.0288      0.0755      0.0741      0.1229 \n",
      "           ##to     0.2620*     0.0797      0.0128      0.0590      0.0950      0.0287      0.0510      0.0039      0.0113      0.0052      0.0114      0.0311      0.1242      0.0114      0.0147      0.0699      0.1286 \n",
      "             se     0.1214      0.1392      0.0211      0.0290      0.2186*     0.0473      0.0444      0.0068      0.0073      0.0021      0.0033      0.0046      0.1846      0.0176      0.0168      0.0305      0.1053 \n",
      "           sent     0.0742      0.0350      0.0193      0.0099      0.1599      0.2560*     0.0910      0.0259      0.0227      0.0031      0.0026      0.0045      0.0967      0.0209      0.0722      0.0647      0.0410 \n",
      "            ##ó     0.1035      0.0672      0.0097      0.0246      0.1934      0.0862      0.0751      0.0116      0.0120      0.0033      0.0050      0.0144      0.2304*     0.0132      0.0207      0.0606      0.0692 \n",
      "             en     0.0593      0.0831      0.0040      0.0054      0.1588      0.1026      0.0579      0.0242      0.0255      0.0051      0.0050      0.0101      0.2626*     0.0220      0.0428      0.0564      0.0752 \n",
      "             la     0.0666      0.0999      0.0118      0.0091      0.0660      0.0535      0.0312      0.0227      0.0687      0.0226      0.0206      0.0261      0.2361*     0.0501      0.0631      0.0535      0.0983 \n",
      "             al     0.0521      0.0750      0.0184      0.0094      0.0356      0.0341      0.0124      0.0332      0.1556      0.0829      0.0584      0.0247      0.0646      0.0872      0.1542      0.0585      0.0436 \n",
      "           ##fo     0.0887      0.0362      0.0199      0.0210      0.0514      0.0509      0.0263      0.0331      0.1161      0.0741      0.0638      0.0700      0.0584      0.0376      0.0881      0.0969      0.0674 \n",
      "         ##mbra     0.1093      0.0524      0.0057      0.0204      0.0604      0.0240      0.0477      0.0348      0.0618      0.0147      0.0291      0.1039      0.1430      0.0195      0.0536      0.1246      0.0952 \n",
      "         porque     0.1123      0.1438      0.0466      0.0722      0.0822      0.0300      0.0347      0.0124      0.0274      0.0101      0.0163      0.0423      0.1644      0.0325      0.0466      0.0665      0.0598 \n",
      "         estaba     0.0644      0.0856      0.0315      0.0277      0.0586      0.0482      0.0128      0.0167      0.0359      0.0173      0.0227      0.0326      0.1495      0.0847      0.1418      0.1300      0.0402 \n",
      "            can     0.0292      0.0201      0.0479      0.0130      0.0297      0.0830      0.0195      0.0195      0.0217      0.0159      0.0174      0.0236      0.0374      0.0586      0.4333*     0.1146      0.0158 \n",
      "         ##sado     0.0719      0.0272      0.0270      0.0369      0.0344      0.0498      0.0315      0.0146      0.0162      0.0164      0.0312      0.0777      0.0658      0.0490      0.1628      0.2504*     0.0374 \n",
      "          [SEP]     0.2912*     0.1545      0.0367      0.0276      0.0485      0.0141      0.0185      0.0052      0.0079      0.0043      0.0059      0.0155      0.1317      0.0108      0.0264      0.0294      0.1718 \n"
     ]
    }
   ],
   "source": [
    "# Comparar patrones en capas tempranas vs profundas\n",
    "print(\"=== CAPAS TEMPRANAS (0-3) ===\")\n",
    "for layer in [0, 2]:\n",
    "    print(f\"\\nCapa {layer}, Cabeza 0:\")\n",
    "    print_attention_weights(attentions_1, tokens_1, layer, 0, threshold=0.2)\n",
    "\n",
    "print(\"\\n\\n=== CAPAS PROFUNDAS (9-11) ===\")\n",
    "for layer in [9, 11]:\n",
    "    print(f\"\\nCapa {layer}, Cabeza 0:\")\n",
    "    print_attention_weights(attentions_1, tokens_1, layer, 0, threshold=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
